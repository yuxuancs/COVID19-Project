{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib as mpl\n",
    "from xgboost import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join as pjoin\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor,KerasClassifier\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics,preprocessing,linear_model,tree\n",
    "from sklearn.model_selection import StratifiedKFold,train_test_split,cross_val_score,KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "from bokeh.io import show, output_notebook, push_notebook\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "from bokeh.models import CategoricalColorMapper, HoverTool, ColumnDataSource, Panel\n",
    "from bokeh.models.widgets import CheckboxGroup, Slider, RangeSlider, Tabs\n",
    "\n",
    "from bokeh.layouts import column, row, WidgetBox\n",
    "from bokeh.palettes import Category20_16\n",
    "\n",
    "from bokeh.application.handlers import FunctionHandler\n",
    "from bokeh.application import Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pd.read_hdf('../data/GSE145926_RAW/GSM4339769_C141_filtered_feature_bc_matrix.h5')\n",
    "# import h5py\n",
    "# path = '../data/GSE145926_RAW/GSM4339769_C141_filtered_feature_bc_matrix.h5'\n",
    "# f = h5py.File(path, 'r')\n",
    "# for key in f.keys():\n",
    "#     print(key)\n",
    "# # group = f['matrix']\n",
    "\n",
    "# # data = []\n",
    "# # for key in group.keys():\n",
    "# #     print(key)\n",
    "# #     data.append(group[(key)].value)\n",
    "# group.keys()\n",
    "# a = group[('features')]\n",
    "# try:\n",
    "#     print(a[()])\n",
    "# except:\n",
    "#     print(a.keys())\n",
    "# a = group[('barcodes')][()]\n",
    "# a.shape\n",
    "# a = group[('data')][()]\n",
    "# a.shape\n",
    "# a = group[('indices')][()]\n",
    "# a.shape\n",
    "# a = group[('indptr')][()]\n",
    "# a.shape\n",
    "# a = group[('shape')][()]\n",
    "# a\n",
    "# a.keys()\n",
    "# a['name'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_time_series():\n",
    "#     # read 375 patients' data, set first column(PATIENT_ID) and second column(RE_DATE) as index (check time)\n",
    "#     data = pd.read_excel('data/time_series_375_prerpocess_en.xlsx', index_col=[0, 1])\n",
    "#     data = data.dropna(thresh=6) #drop out 14 rows\n",
    "#     data.to_parquet('data/time_series_375.parquet')\n",
    "#     # read 100 patients' data, set first column (PATIENT_ID) and second column (RE_DATE) as index\n",
    "#     data = pd.read_excel('data/time_series_test_110_preprocess_en.xlsx', index_col=[0, 1])\n",
    "#     data.to_parquet('data/time_series_test_110.parquet')\n",
    "\n",
    "# # preprocess_time_series()\n",
    "\n",
    "def preprocessTimeSeries(path, drop=False):\n",
    "    '''\n",
    "    read excel file and preprocess times series using parquet, if drop is true then drop NA rows\n",
    "    '''\n",
    "    data = pd.read_excel(path, index_col=[0, 1])\n",
    "    if drop:\n",
    "        data = data.dropna(thresh=6) #drop out 14 rows\n",
    "    data.to_parquet(path.split('.')[0]+'.parquet')\n",
    "    \n",
    "# preprocessTimeSeries('data/time_series_375_prerpocess_en.xlsx',True)\n",
    "# preprocessTimeSeries('data/time_series_test_110_preprocess_en.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTimeSeries(path,col=None):\n",
    "    if col == None:\n",
    "        data = pd.read_parquet(path)\n",
    "    else:\n",
    "        data = pd.read_parquet(path)[col]\n",
    "    return data\n",
    "\n",
    "# col = ['age', 'gender','Lactate dehydrogenase','High sensitivity C-reactive protein','(%)lymphocyte',\n",
    "#        'Discharge time', 'outcome']\n",
    "# data1 = readTimeSeries('data/time_series_375_prerpocess_en.parquet',col)\n",
    "col = ['Lactate dehydrogenase','High sensitivity C-reactive protein','(%)lymphocyte',\n",
    "       'Discharge time', 'outcome']\n",
    "# data1 = readTimeSeries('data/time_series_375_prerpocess_en.parquet',col)\n",
    "# data2 = readTimeSeries('data/time_series_test_110_preprocess_en.parquet',col)\n",
    "data1 = readTimeSeries('data/time_series_375_prerpocess_en.parquet')\n",
    "data2 = readTimeSeries('data/time_series_test_110_preprocess_en.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenateData(data1: pd.DataFrame, data2: pd.DataFrame):\n",
    "    '''\n",
    "    concatenate two datasets\n",
    "    PATIENT_ID starts from 1, needs to reset index in order to aviod duplicate\n",
    "\n",
    "    Arg:\n",
    "        data1: DataFrame\n",
    "        data2: DataFrame\n",
    "    Return:\n",
    "        data\n",
    "    \n",
    "    '''\n",
    "    data2 = data2.reset_index()\n",
    "    data2['PATIENT_ID'] += data1.index[-1][0]\n",
    "    data2 = data2.set_index(['PATIENT_ID', 'RE_DATE'])\n",
    "    return data1.append(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self, report=None, acc=None, f1=None, conf_mat=None):\n",
    "        self.y_trues  = []\n",
    "        self.y_preds  = []\n",
    "        \n",
    "        # list or None. every: each; 'overall': print all\n",
    "        if isinstance(report, list):\n",
    "            self.report = report\n",
    "        else:\n",
    "            self.report = [report]\n",
    "\n",
    "        if isinstance(acc, list):\n",
    "            self.acc = acc\n",
    "        else:\n",
    "            self.acc = [acc]\n",
    "\n",
    "        if isinstance(f1, list):\n",
    "            self.f1 = f1\n",
    "        else:\n",
    "            self.f1 = [f1]\n",
    "\n",
    "        if isinstance(conf_mat, list):\n",
    "            self.conf_mat = conf_mat\n",
    "        else:\n",
    "            self.conf_mat = [conf_mat]\n",
    "\n",
    "    def record(self, y_true, y_pred):\n",
    "        self.y_trues.append(y_true)\n",
    "        self.y_preds.append(y_pred)\n",
    "        return self\n",
    "\n",
    "    def clear(self):\n",
    "        self.y_trues = []\n",
    "        self.y_preds = []\n",
    "        return self\n",
    "\n",
    "    def print_metrics(self):\n",
    "        \"\"\"\n",
    "        :param report:\n",
    "        :param acc:\n",
    "        :param f1:\n",
    "        :param conf_mat:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Loop: 'every'\n",
    "        acc_values, f1_values = [], []\n",
    "        single_fold = True if len(self.y_trues) == 1 else False\n",
    "        for i, (y_true, y_pred) in enumerate(zip(self.y_trues, self.y_preds)):\n",
    "            assert (y_true.ndim == 1) and (y_pred.ndim == 1)\n",
    "            print(f'\\n========================  {i+1}  ========================>')\n",
    "\n",
    "            # Classification_report\n",
    "            if (self.report is not None) and ('every' in self.report):\n",
    "                print(metrics.classification_report(y_true, y_pred))\n",
    "\n",
    "            # Accuracy_score\n",
    "            a_v = metrics.accuracy_score(y_true, y_pred)\n",
    "            acc_values.append(a_v)\n",
    "            if (self.acc is not None) and ('every' in self.acc):\n",
    "                print(f\"accuracy: {a_v:.05f}\")\n",
    "\n",
    "            # F1_score\n",
    "            f1_v = metrics.f1_score(y_true, y_pred, average='macro')\n",
    "            f1_values.append(f1_v)\n",
    "            if (self.f1 is not None) and ('every' in self.f1):\n",
    "                print(f\"F1: {f1_v:.05f}\")\n",
    "\n",
    "            # Confusion_matrix\n",
    "            if (self.conf_mat is not None) and ('every' in self.conf_mat):\n",
    "                print(f\"Confusion Matrix：\\n{metrics.confusion_matrix(y_true, y_pred)}\")\n",
    "\n",
    "        # 'Overall'\n",
    "        print('\\n======================== all ========================>')\n",
    "        y_true = np.hstack(self.y_trues)\n",
    "        y_pred = np.hstack(self.y_preds)\n",
    "\n",
    "        # Classification_report\n",
    "        if (self.report is not None) and ('overall' in self.report):\n",
    "            print(metrics.classification_report(y_true, y_pred))\n",
    "        # Accuracy_score\n",
    "        if (self.acc is not None) and ('overall' in self.acc):\n",
    "            if single_fold:\n",
    "                print(f\"accuracy：\\t{acc_values[0]: .04f}\")\n",
    "            else:\n",
    "                print(f\"accuracy：\\t{np.mean(acc_values): .04f}/ \\\n",
    "                      {'  '.join([str(a_v.round(2)) for a_v in acc_values])}\")\n",
    "        # F1_score\n",
    "        if (self.f1 is not None) and ('overall' in self.f1):\n",
    "            if single_fold:\n",
    "                print(f\"F1-score：\\t{f1_values[0]: .04f}\")\n",
    "            else:\n",
    "                print(f\"F1 score：\\t{np.mean(f1_values): .04f}/ \\\n",
    "                      {'  '.join([str(f1_v.round(2)) for f1_v in f1_values])}\")\n",
    "\n",
    "        # Confusion_matrix\n",
    "        if (self.conf_mat is not None) and ('overall' in self.conf_mat):\n",
    "            print(f\"Confusion Matrix：\\n{confusion_matrix(y_true, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeDataBySlidingWindow(data, nDays=1,dropna=True,subset=col[:3],timeForm='diff'):\n",
    "    \"\"\"滑窗合并数据\n",
    "\n",
    "    :param data: 时间序列数据，一级行索引为 PATIENT_ID, 二级行索引为 RE_DATE\n",
    "    :param n_days: 窗口长度\n",
    "    :param dropna: 滑窗合并后还缺失的是否删掉\n",
    "    :param subset: pd.DataFrame().dropna() 参数                   Note: 新参数!\n",
    "    :param time_form: 返回数据的时间索引，'diff' or 'timestamp'\n",
    "    :return: 合并后的数据，一级行索引为 PATIENT_ID, 二级行索引为 t_diff or RE_DATE, 取决于\"time_form\"\n",
    "    \"\"\"\n",
    "    #根据PATIENT_ID排序\n",
    "    data = data.reset_index(level=1)\n",
    "    # dt.normalize() 取Discharge time的天数\n",
    "    # 距离出院时长        Note: 去掉了Discharge time和检测时间的时分秒，因为我觉得以 00:00:00 为分界点更合适\n",
    "    t_diff = data['Discharge time'].dt.normalize() - data['RE_DATE'].dt.normalize()\n",
    "    # 滑窗取整的依据。即nn_days天内的会取整成为同一个数值，后面通过groupby方法分组\n",
    "    data['t_diff'] = t_diff.dt.days.values // nDays * nDays\n",
    "    #\n",
    "    data = data.set_index('t_diff', append=True)\n",
    "\n",
    "    # 滑窗合并。对['PATIENT_ID', 't_diff']groupby，相当于双循环。遍历所有病人与病人的所有窗口\n",
    "    # 因为之前对data排序，因此每个病人t_diff会是从大到小的排序,ffill()是向上一行插值，因此相当于是向旧日期插值\n",
    "    # last()是每一组取最后一行，因此即取每个病人对应窗口的最后一次数据，（也一定是最全的）。\n",
    "    # last(）自带排序。取完last后会按照索引升序排列\n",
    "    data = (data.groupby(['PATIENT_ID', 't_diff']).ffill()\n",
    "                .groupby(['PATIENT_ID', 't_diff']).last())\n",
    "    # 去掉缺失样本\n",
    "    if dropna:\n",
    "        data = data.dropna(subset=subset)  # Note: 这里对缺失值进行了 dropna(), 而不是 fillna(-1)\n",
    "\n",
    "    # 更新二级索引。（其实timestamp在本论文的中没用到）\n",
    "    if timeForm == 'timestamp':\n",
    "        data = (data.reset_index(level=1, drop=True)\n",
    "                    .set_index('RE_DATE', append=True))\n",
    "    elif timeForm == 'diff':\n",
    "        data = data.drop(columns=['RE_DATE'])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHist(ax,data,**kwargs):\n",
    "    # d = data.reset_index().groupby(['PATIENT_ID','age'],as_index=False).groups.keys()\n",
    "    # data.reset_index().groupby(['PATIENT_ID','age'],as_index=False).agg('mean')\n",
    "    # data.loc[data['age']<30,'group_age'] = 'young'\n",
    "    # data.loc[(data['age']>=30)&(data['age']<60),'group_age'] = 'middile'\n",
    "    # data.loc[(data['age']>=60),'group_age'] = 'old'\n",
    "    params = dict(density=True,stacked=True,histtype='barstacked',label=['die','survial'],\n",
    "                  xlabel='Age',ylabel='Percentage',bins=np.linspace(0,100,21,dtype=int))\n",
    "    params.update(kwargs)\n",
    "    # plt.rcParams.update({'font.size': 25})\n",
    "    xticks = np.linspace(0,100,21,dtype=int)\n",
    "    bins = params.get('bins')\n",
    "    hist = ax.hist(data,bins=bins,density=params.get('density'),stacked=params.get('stacked'),\n",
    "                   histtype=params.get('histtype'),alpha=0.75,label=params.get('label'),\n",
    "                   rwidth=0.98)\n",
    "    kde = stats.gaussian_kde(np.concatenate(data, axis=None))\n",
    "#     ax.plot(bins,kde.pdf(bins),label='KDE',lw=3) # kernel density estimation\n",
    "    ax.plot(bins,kde.pdf(bins)*kde.n*np.diff(bins)[0],label='KDE',lw=3)\n",
    "    ax.set(xlabel=params.get('xlabel'),xticks=bins,xticklabels=xticks,ylabel=params.get('ylabel'))\n",
    "\n",
    "    for i,rect in enumerate(ax.patches):\n",
    "    #     height = rect.get_height()\n",
    "    #     ax.annotate(f'{np.round(height,4)}', xy=(rect.get_x()+rect.get_width()/2,height), \n",
    "    #                 xytext=(0,5),textcoords='offset points',ha='center', va='bottom')\n",
    "        width, height = rect.get_width(), rect.get_height()\n",
    "        if height <= 0:\n",
    "            continue\n",
    "        x, y = rect.get_xy() \n",
    "        ax.text(x+width/2,y+height/2, '{:.4f}'.format(height), \n",
    "                horizontalalignment='center', verticalalignment='top')\n",
    "    ax.legend(fontsize='large')\n",
    "    fig.subplots_adjust(wspace=0,hspace=0)\n",
    "    fig.savefig('age&gender.jpeg')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['age','Lactate dehydrogenase','High sensitivity C-reactive protein','(%)lymphocyte','creatinine']\n",
    "data = mergeDataBySlidingWindow(data1, nDays=1,dropna=True,subset=col,timeForm='diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(figsize=(50,50))\n",
    "# # cm = data1.corr()\n",
    "# cm = a\n",
    "# sns.set(font_scale=3)\n",
    "# hm = sns.heatmap(cm,cbar=True,annot=True,square=True,fmt='0.3f',annot_kws={'size':5},\n",
    "#                  cmap='rainbow_r',ax=ax,\n",
    "#                  yticklabels=data1.columns,xticklabels=data1.columns)\n",
    "# ax.set_title('Corrolation between 5 features and outcome')\n",
    "# fig.savefig('heatmap.jpeg')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data.reset_index().drop(\n",
    "    labels=['Admission time','Discharge time','t_diff'],axis=1).groupby('PATIENT_ID').agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data1.reset_index(level=0).groupby('PATIENT_ID').agg('mean').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (pd.read_csv('data/Data (processed).csv')\n",
    "     .rename(columns={'Outcome':'outcome'})\n",
    "     .drop(labels='PATIENT_ID',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = data.drop(labels=['Admission time','Discharge time'],axis=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(a.drop(labels='outcome',axis=1),a['outcome'],\n",
    "                                                  test_size=0.3, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123, 75)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64,input_dim=75,kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(16,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "# model.add(Dense(1,kernel_initializer='normal'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy',optimizer='adam')\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f9daef7d990>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train,y=y_train,batch_size=20,epochs=100,verbose=0,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9245283018867925"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "y_pred[y_pred >0.5]=1\n",
    "y_pred[y_pred<=0.5]=0\n",
    "metrics.accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=model)\n",
    "kfold=StratifiedKFold(n_splits=10,shuffle=True)\n",
    "cross_val_score(estimator,X_val,y_val,cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f9ff8265fd0>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM(units = 50,return_sequences = True,input_shape = (1,75)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "regressor.compile(optimizer='adam',loss='binary_crossentropy')\n",
    "\n",
    "regressor.fit(X_train.values.reshape(123,1,75), y_train, epochs = 100,verbose=0,batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 5s 100ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9205222545929674"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.evaluate(X_val.values.reshape(53,1,75),y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons):\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(y.shape[1]))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    # fit network\n",
    "    for i in range(nb_epoch):\n",
    "        model.fit(X, y, epochs=1, batch_size=n_batch, verbose=0, shuffle=False)\n",
    "        model.reset_states()\n",
    "    return model\n",
    "\n",
    "# make one forecast with an LSTM,\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    # make forecast\n",
    "    forecast = model.predict(X, batch_size=n_batch)\n",
    "    # convert to array\n",
    "    return [x for x in forecast[0, :]]\n",
    "\n",
    "# evaluate the persistence model\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n",
    "    forecasts = list()\n",
    "    for i in range(len(test)):\n",
    "        X, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_batch)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_doc(doc):\n",
    "    \n",
    "    def make_dataset(carrier_list, range_start = -60, range_end = 120, bin_width = 5):\n",
    "\n",
    "        by_carrier = pd.DataFrame(columns=['proportion', 'left', 'right', \n",
    "                                           'f_proportion', 'f_interval',\n",
    "                                           'name', 'color'])\n",
    "        range_extent = range_end - range_start\n",
    "\n",
    "        # Iterate through all the carriers\n",
    "        for i, carrier_name in enumerate(carrier_list):\n",
    "\n",
    "            # Subset to the carrier\n",
    "            subset = flights[flights['name'] == carrier_name]\n",
    "\n",
    "            # Create a histogram with 5 minute bins\n",
    "            arr_hist, edges = np.histogram(subset['arr_delay'], \n",
    "                                           bins = int(range_extent / bin_width), \n",
    "                                           range = [range_start, range_end])\n",
    "\n",
    "            # Divide the counts by the total to get a proportion\n",
    "            arr_df = pd.DataFrame({'proportion': arr_hist / np.sum(arr_hist), 'left': edges[:-1], 'right': edges[1:] })\n",
    "\n",
    "            # Format the proportion \n",
    "            arr_df['f_proportion'] = ['%0.5f' % proportion for proportion in arr_df['proportion']]\n",
    "\n",
    "            # Format the interval\n",
    "            arr_df['f_interval'] = ['%d to %d minutes' % (left, right) for left, right in zip(arr_df['left'], arr_df['right'])]\n",
    "\n",
    "            # Assign the carrier for labels\n",
    "            arr_df['name'] = carrier_name\n",
    "\n",
    "            # Color each carrier differently\n",
    "            arr_df['color'] = Category20_16[i]\n",
    "\n",
    "            # Add to the overall dataframe\n",
    "            by_carrier = by_carrier.append(arr_df)\n",
    "\n",
    "        # Overall dataframe\n",
    "        by_carrier = by_carrier.sort_values(['name', 'left'])\n",
    "\n",
    "        return ColumnDataSource(by_carrier)\n",
    "    \n",
    "    def style(p):\n",
    "        # Title \n",
    "        p.title.align = 'center'\n",
    "        p.title.text_font_size = '20pt'\n",
    "        p.title.text_font = 'serif'\n",
    "\n",
    "        # Axis titles\n",
    "        p.xaxis.axis_label_text_font_size = '14pt'\n",
    "        p.xaxis.axis_label_text_font_style = 'bold'\n",
    "        p.yaxis.axis_label_text_font_size = '14pt'\n",
    "        p.yaxis.axis_label_text_font_style = 'bold'\n",
    "\n",
    "        # Tick labels\n",
    "        p.xaxis.major_label_text_font_size = '12pt'\n",
    "        p.yaxis.major_label_text_font_size = '12pt'\n",
    "\n",
    "        return p\n",
    "    \n",
    "    def make_plot(src):\n",
    "        # Blank plot with correct labels\n",
    "        p = figure(plot_width = 700, plot_height = 700, \n",
    "                  title = 'Histogram of Arrival Delays by Carrier',\n",
    "                  x_axis_label = 'Delay (min)', y_axis_label = 'Proportion')\n",
    "\n",
    "        # Quad glyphs to create a histogram\n",
    "        p.quad(source = src, bottom = 0, top = 'proportion', left = 'left', right = 'right',\n",
    "               color = 'color', fill_alpha = 0.7, hover_fill_color = 'color', legend = 'name',\n",
    "               hover_fill_alpha = 1.0, line_color = 'black')\n",
    "\n",
    "        # Hover tool with vline mode\n",
    "        hover = HoverTool(tooltips=[('Carrier', '@name'), \n",
    "                                    ('Delay', '@f_interval'),\n",
    "                                    ('Proportion', '@f_proportion')],\n",
    "                          mode='vline')\n",
    "\n",
    "        p.add_tools(hover)\n",
    "\n",
    "        # Styling\n",
    "        p = style(p)\n",
    "\n",
    "        return p\n",
    "    \n",
    "    def update(attr, old, new):\n",
    "        carriers_to_plot = [carrier_selection.labels[i] for i in carrier_selection.active]\n",
    "        \n",
    "        new_src = make_dataset(carriers_to_plot,\n",
    "                               range_start = range_select.value[0],\n",
    "                               range_end = range_select.value[1],\n",
    "                               bin_width = binwidth_select.value)\n",
    "\n",
    "        src.data.update(new_src.data)\n",
    "\n",
    "        \n",
    "    carrier_selection = CheckboxGroup(labels=available_carriers, active = [0, 1])\n",
    "    carrier_selection.on_change('active', update)\n",
    "    \n",
    "    binwidth_select = Slider(start = 1, end = 30, \n",
    "                         step = 1, value = 5,\n",
    "                         title = 'Delay Width (min)')\n",
    "    binwidth_select.on_change('value', update)\n",
    "    \n",
    "    range_select = RangeSlider(start = -60, end = 180, value = (-60, 120),\n",
    "                               step = 5, title = 'Delay Range (min)')\n",
    "    range_select.on_change('value', update)\n",
    "    \n",
    "    \n",
    "    \n",
    "    initial_carriers = [carrier_selection.labels[i] for i in carrier_selection.active]\n",
    "    \n",
    "    src = make_dataset(initial_carriers,\n",
    "                      range_start = range_select.value[0],\n",
    "                      range_end = range_select.value[1],\n",
    "                      bin_width = binwidth_select.value)\n",
    "    \n",
    "    p = make_plot(src)\n",
    "    \n",
    "    # Put controls in a single element\n",
    "    controls = WidgetBox(carrier_selection, binwidth_select, range_select)\n",
    "    \n",
    "    # Create a row layout\n",
    "    layout = row(controls, p)\n",
    "    \n",
    "    # Make a tab with the layout \n",
    "    tab = Panel(child=layout, title = 'Delay Histogram')\n",
    "    tabs = Tabs(tabs=[tab])\n",
    "    \n",
    "    doc.add_root(tabs)\n",
    "    \n",
    "# Set up an application\n",
    "handler = FunctionHandler(modify_doc)\n",
    "app = Application(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mergeDataBySlidingWindow(data1, nDays=1,dropna=False,subset=None,timeForm='diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['age', 'gender','Lactate dehydrogenase','High sensitivity C-reactive protein','(%)lymphocyte',\n",
    "       'Discharge time', 'outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get the group key, get age, gender and outcomes\n",
    "'''\n",
    "# d = data.reset_index().groupby(['PATIENT_ID','age'],as_index=False).groups.keys()\n",
    "# data.reset_index().groupby(['PATIENT_ID','age'],as_index=False).agg('mean')\n",
    "# data.loc[data['age']<30,'group_age'] = 'young'\n",
    "# data.loc[(data['age']>=30)&(data['age']<60),'group_age'] = 'middile'\n",
    "# data.loc[(data['age']>=60),'group_age'] = 'old'\n",
    "getdf = data.reset_index().groupby(['PATIENT_ID','age'],as_index=False).agg('mean')\n",
    "get_age = getdf[['age', 'gender','outcome']].values\n",
    "stdScaler = preprocessing.StandardScaler()\n",
    "c = np.hstack((stdScaler.fit_transform(get_age[:,0,None]),get_age[:,1:]))\n",
    "plt.rcParams.update({'font.size': 35})\n",
    "cond1 = c[:,1] ==1\n",
    "cond2 = c[:,2] ==0\n",
    "xticks = np.linspace(0,100,21,dtype=int)\n",
    "bins = stdScaler.transform(xticks[:,None]).flatten()\n",
    "fig,ax=plt.subplots(figsize=(30,20),dpi=300)\n",
    "# hist = ax.hist((c[np.where((c[:,1]==1)&(c[:,2]==1))[0],0],c[np.where((c[:,1]==2)&(c[:,2]==1))[0],0],\n",
    "#                 c[np.where((c[:,1]==1)&(c[:,2]==0))[0],0],c[np.where((c[:,1]==2)&(c[:,2]==0))[0],0]),\n",
    "#                density=True,stacked=True,bins=bins,histtype='barstacked',alpha=0.75,\n",
    "#                label=['M die','F die','M survial','F survial'])\n",
    "hist = ax.hist((c[np.where(cond1&~cond2)[0],0],c[np.where(~cond1&~cond2)[0],0],\n",
    "                c[np.where(cond1&cond2)[0],0],c[np.where(~cond1&cond2)[0],0]),\n",
    "#                density=True,stacked=True,\n",
    "               bins=bins,rwidth=0.98,histtype='barstacked',alpha=0.75,\n",
    "               label=['M die','F die','M survial','F survial'])\n",
    "kde = stats.gaussian_kde(c[:,0])\n",
    "ax.plot(bins,kde.pdf(bins)*kde.n*np.diff(bins)[0],label='KDE',lw=3)\n",
    "\n",
    "for i,rect in enumerate(ax.patches):\n",
    "#     height = rect.get_height()\n",
    "#     ax.annotate(f'{np.round(height,4)}', xy=(rect.get_x()+rect.get_width()/2,height), \n",
    "#                 xytext=(0,5),textcoords='offset points',ha='center', va='bottom') \n",
    "    width, height = rect.get_width(), rect.get_height()\n",
    "    if height <= 0:\n",
    "        continue\n",
    "    x, y = rect.get_xy() \n",
    "    ax.text(x+width/2,y+height/2,'{:3.0f}'.format(height), \n",
    "            horizontalalignment='center', verticalalignment='center')\n",
    "ax.set(xticks=bins, xticklabels=xticks,xlabel='Age',ylabel='counts')\n",
    "# ax.set_yticklabels(labels=ax.get_yticks()*100)\n",
    "ax.legend(fontsize='xx-large')\n",
    "ax.grid(b=True,axis='y')\n",
    "# fig.subplots_adjust(wspace=0,hspace=0)\n",
    "fig.savefig('age&gender.jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I have reviewed your visualizations. Some of them are quite nice, and some need clarifications.\n",
    "\n",
    "1. need to be more specific about training and testing samples. How were accuracies calculated?\n",
    "\n",
    "2. Among the ratios, which were important? How many of them seem to be enough for predicting mortality?\n",
    "\n",
    "3. Can the analysis be done separately for different age groups (young, middle age, old) and different disease severity (general, severe, critical)?\n",
    "\n",
    "4. In the decision trees, the labels of the nodes and edges are not clear to me.  These should be simplified so that biologists and doctors can understand. See Figure 2 of the original paper. This is an important visualization.\n",
    "\n",
    "5. In the neural network approach, instead of taking only age and gender can you consider other predictors also? Can you compare with the observed proportion of deaths in the different age groups?\n",
    "\n",
    "6. The 3-d plots are also interesting. Does the clustering algorithm perform well? What are the proportion of misclassification?\n",
    "\n",
    "7. Was the severity status determined once at the beginning? if not, can we predict the severity status (mild, severe, critical) based on the protein measurements and their ratios of the initial blood sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(30,20),dpi=300)\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "# plotHist(ax,c[np.where(c[:,1]==0)[0],0],label='survial')\n",
    "# fig,ax=plt.subplots(figsize=(30,20),dpi=300)\n",
    "# plotHist(ax,c[np.where(c[:,1]==1)[0],0],label='die')\n",
    "# fig,ax=plt.subplots(figsize=(30,20),dpi=300)\n",
    "plotHist(ax,(c[np.where(c[:,1]==1)[0],0],c[np.where(c[:,1]==0)[0],0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset\n",
    "dataframe = read_csv('airline-passengers.csv', usecols=[1], engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(look_back, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,data in enumerate([data1, data2, concatenateData(data1,data2)]):\n",
    "    data = mergeDataBySlidingWindow(data, nDays=1,dropna=True,subset=col[:3],timeForm='diff')\n",
    "    classifier = tree.DecisionTreeClassifier()\n",
    "    if i == 0:\n",
    "        classifier.fit(data1[col[:3]].values, data1[col[-1]].values)\n",
    "    \n",
    "    data['pred'] = data.apply()\n",
    "    resultMetric = Metrics(*(['overall']*4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['age', 'gender','Lactate dehydrogenase','High sensitivity C-reactive protein','(%)lymphocyte',\n",
    "       'Discharge time', 'outcome']\n",
    "data = mergeDataBySlidingWindow(data1, nDays=1,dropna=True,subset=col[:3],timeForm='diff')\n",
    "data3 = mergeDataBySlidingWindow(data2, nDays=1,dropna=True,subset=col[:3],timeForm='diff')\n",
    "X_train=data[col[:3]].values\n",
    "X_test=data3[col[:3]].values\n",
    "y_train = data[col[-1]].values\n",
    "y_true = data3[col[-1]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [KNeighborsClassifier(2),\n",
    "               RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "               MLPClassifier(alpha=1, max_iter=1000),\n",
    "               AdaBoostClassifier()]\n",
    "names = ['knn','Random Forest','MLP','AdaBoost']\n",
    "for name, clf in zip(names,classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    print('='*20+name+'='*20)\n",
    "    print(metrics.confusion_matrix(y_true, y_pred))\n",
    "    print(metrics.classification_report(y_true, y_pred))\n",
    "    print(metrics.accuracy_score(y_true, y_pred))\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col = ['age', 'gender','Lactate dehydrogenase','High sensitivity C-reactive protein','(%)lymphocyte',\n",
    "#        'serum sodium','Discharge time', 'outcome']\n",
    "col = ['Lactate dehydrogenase','High sensitivity C-reactive protein','(%)lymphocyte','outcome']\n",
    "data = mergeDataBySlidingWindow(data1, nDays=1,dropna=True,subset=col[:-1],timeForm='diff')\n",
    "data3 = mergeDataBySlidingWindow(data2, nDays=1,dropna=True,subset=col[:-1],timeForm='diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "k fold 07/11/2020\n",
    "'''\n",
    "all_histories = []\n",
    "k = 4\n",
    "X_train = data[col[:-1]].values\n",
    "y_train = data[col[-1]]\n",
    "X_val = data1[col[:-1]].values\n",
    "y_val = data1[col[-1]]\n",
    "num_val_samples = data.shape[0]//k\n",
    "for i in range(k):\n",
    "    print('processing fold #',i)\n",
    "    ind = i*num_val_samples\n",
    "    val_data = X_train[ind:ind+num_val_samples]\n",
    "    val_targets = y_train[ind:ind+num_val_samples]\n",
    "    partial_train_data = np.concatenate([X_train[:ind],X_train[ind+num_val_samples:]],axis=0)\n",
    "    partial_train_targets = np.concatenate([y_train[:ind],y_train[ind+num_val_samples:]],axis=0)\n",
    "    classifier = tree.DecisionTreeClassifier()\n",
    "    classifier.fit(partial_train_data,partial_train_targets)\n",
    "    y_pred=classifier.predict(X_val)\n",
    "    print(metrics.confusion_matrix(y_val, y_pred))\n",
    "    print(metrics.classification_report(y_val, y_pred))\n",
    "    print('='*50)\n",
    "    all_histories.append(metrics.accuracy_score(y_val, y_pred))\n",
    "np.sum(all_histories)/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(data3[col[:5]].values,data3[col[-1]],\n",
    "                                                  test_size=0.3, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = data[col[:-1]],data[col[-1]]\n",
    "X_val,y_val = data3[col[:-1]],data3[col[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(40,50),dpi=300)\n",
    "plotT = tree.plot_tree(classifier,feature_names=col[:-1],class_names=True,filled=True,\n",
    "                       rounded=True,ax=ax,fontsize=5)\n",
    "ax.text(0,1,'Right:No, Left:Yes\\nFeatures:\\n1.{},\\n2.{},\\n3.{}\\n \\\n",
    "            outcome: die or survial'.format(*col[:-1]),fontsize=50,\n",
    "        horizontalalignment='center',verticalalignment='center')\n",
    "fig.savefig('tree.jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[173   9]\n",
      " [ 14  55]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94       182\n",
      "           1       0.86      0.80      0.83        69\n",
      "\n",
      "    accuracy                           0.91       251\n",
      "   macro avg       0.89      0.87      0.88       251\n",
      "weighted avg       0.91      0.91      0.91       251\n",
      "\n",
      "0.9083665338645418\n"
     ]
    }
   ],
   "source": [
    "y_pred=classifier.predict(X_val)\n",
    "print(metrics.confusion_matrix(y_val, y_pred))\n",
    "print(metrics.classification_report(y_val, y_pred))\n",
    "print(metrics.accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.19999822,  2.        ,  3.        ],\n",
       "       [ 4.        , -0.9999978 ,  6.        ],\n",
       "       [10.        ,  4.99999933,  9.        ]])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "imp_mean = IterativeImputer(random_state=0)\n",
    "imp_mean.fit([[7, 2, np.nan],\n",
    "              [4, np.nan, 6],\n",
    "              [10, 5, 9]])\n",
    "\n",
    "X = [[np.nan, 2, 3],\n",
    "     [4, np.nan, 6],\n",
    "     [10, np.nan, 9]]\n",
    "imp_mean.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(3,3),dpi=300)\n",
    "# plt.rcParams.update({'font.size': 5})\n",
    "# pltcm = metrics.plot_confusion_matrix(classifier,X_val,y_val,values_format='d',\n",
    "#                               labels=[0,1],display_labels=['surival','die'],ax=ax)\n",
    "# pltcm.ax_.set_title('accuracy: %s %%'% np.round(metrics.accuracy_score(y_val, y_pred)*100,2))\n",
    "# pltcm.ax_.text(-0.5,-0.3,'Features:\\n1.{},\\n2.{},\\n3.{}'.format(*col[:-1]),\n",
    "#         horizontalalignment='center',verticalalignment='center')\n",
    "# pltcm.ax_.set_aspect('auto')\n",
    "# # ax.set_xlabel('Predict Label',fontdict={'size':font})\n",
    "# # ax.set_ylabel('True Label',fontdict={'size':font})\n",
    "# plt.tight_layout(h_pad=1)\n",
    "# fig.savefig('confusion_matrix.jpeg')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 40})\n",
    "confusionMatrix = metrics.confusion_matrix(y_val, y_pred)\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "sns.set(font_scale=3.5)\n",
    "xcol=['Surival','Death']\n",
    "ax = sns.heatmap(confusionMatrix,cmap='cool',annot=True,cbar=True,\n",
    "                 annot_kws={'size':80},\n",
    "                 linewidths=.5,fmt='d',xticklabels=xcol,yticklabels=xcol,\n",
    "                 ax=ax)\n",
    "ax.set(xlabel='Predicted label',ylabel='True label')\n",
    "ax.set_title('accuracy: %s %%'% np.round(metrics.accuracy_score(y_val, y_pred)*100,2))\n",
    "# ax.text(0,0,'Features:\\n1.{},\\n2.{},\\n3.{}'.format(*col[:-1]),\n",
    "#         horizontalalignment='center',verticalalignment='center')\n",
    "ax.set_aspect('auto')\n",
    "# plt.tight_layout(h_pad=1)\n",
    "fig.savefig('confusion_matrix_3.jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from graphviz import Digraph\n",
    "# dot = Digraph(comment='Simple Decision Tree')\n",
    "# dot.node('A','Lactate dehydrogenase < 365')\n",
    "# dot.node('B','High sensitivity C-reactive protein')\n",
    "# dot.node('C','(%)lymphocyte < 41.2')\n",
    "# dot.node('D','(%)lymphocyte < 21.4')\n",
    "# dot.edges(['AC','AD'])\n",
    "# dot.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=classifier.predict(data3[col[:3]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mergeDataBySlidingWindow(data1, nDays=1,dropna=True,subset=col[:5],timeForm='diff')\n",
    "classifier = tree.DecisionTreeClassifier()\n",
    "X_train, X_val, y_train, y_val = train_test_split(data[col[:5]].values,data[col[-1]],\n",
    "                                                  test_size=0.3, random_state=6)\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred=classifier.predict(X_val)\n",
    "print(metrics.confusion_matrix(y_val, y_pred))\n",
    "print(metrics.classification_report(y_val, y_pred))\n",
    "print(metrics.accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(data3[col[-1]], y_pred))\n",
    "print(metrics.classification_report(data3[col[-1]], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['age', 'gender','Lactate dehydrogenase','High sensitivity C-reactive protein',\n",
    "        '(%)lymphocyte','outcome']\n",
    "newdata = data[col].dropna(thresh=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "listPlot = []\n",
    "for i in range(2):\n",
    "    for j in range(1,3):\n",
    "        listPlot.append(newdata.loc[(newdata.gender==j)&(newdata.outcome==i)][col[2:-1]])\n",
    "# newdata.loc[(newdata.gender==1)&(newdata.outcome==0)][col[2:-1]]\n",
    "# newdata.loc[(newdata.gender==2)&(newdata.outcome==0)][col[2:-1]]\n",
    "# newdata.loc[(newdata.gender==1)&(newdata.outcome==1)][col[2:-1]]\n",
    "# newdata.loc[(newdata.gender==2)&(newdata.outcome==1)][col[2:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 35})\n",
    "label = ['M-survival','F-survival','M-death','F-death']\n",
    "fig = plt.figure(figsize=(20,20),dpi=300)\n",
    "ax = plt.axes(projection='3d')\n",
    "for i, mk in enumerate(['.','v','s','*']):\n",
    "    ax.scatter3D(*listPlot[i].values.T,marker=mk,linewidths=10,label=label[i])\n",
    "# ax.set(xlabel=col[2],ylabel=col[3],zlabel=col[4])\n",
    "ax.set_xlabel(col[2],labelpad=30)\n",
    "ax.set_ylabel(col[3],labelpad=30)\n",
    "ax.set_zlabel(col[4],labelpad=25)\n",
    "ax.set_title('Using three features for gender and its outcome')\n",
    "ax.legend(loc=(0.5,0.6),fontsize='xx-large',markerscale=1.5)\n",
    "ax.set_aspect('auto')\n",
    "plt.tight_layout()\n",
    "fig.savefig('3dplot.jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(style='whitegrid',context='notebook')\n",
    "# sns.pairplot(listPlot[0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 40})\n",
    "col = ['age', 'gender','LD','hs-crp','(%)lymphocyte','outcome']\n",
    "fig,ax = plt.subplots(figsize=(30,30))\n",
    "cm = newdata.corr()\n",
    "sns.set(font_scale=4)\n",
    "hm = sns.heatmap(cm,cbar=True,annot=True,square=True,fmt='0.3f',\n",
    "                 cmap='rainbow_r',ax=ax,linewidths=0.5,\n",
    "                 annot_kws={'size':70},\n",
    "#                  yticklabels=newdata.columns.values,xticklabels=newdata.columns.values)\n",
    "                 yticklabels=col,xticklabels=col)\n",
    "ax.set_title('Corrolation between 5 features and outcome')\n",
    "ax.set_aspect('auto')\n",
    "plt.tight_layout()\n",
    "fig.savefig('heatmap.jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y = listPlot[i].values.T[:2]\n",
    "# X,Y = np.meshgrid(x,y)\n",
    "# Z = \n",
    "# plt.rcParams.update({'font.size': 20})\n",
    "# label = ['M-survial','F-survial','M-death','F-death']\n",
    "# fig = plt.figure(figsize=(20,20),dpi=300)\n",
    "# ax = plt.axes(projection='3d')\n",
    "# ax.contour3D(X,Y,Z)\n",
    "# ax.set_xlabel(col[2],labelpad=20)\n",
    "# ax.set_ylabel(col[3],labelpad=20)\n",
    "# ax.set_zlabel(col[4],labelpad=10)\n",
    "# ax.set_title('Using three features for gender and its outcome')\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array\n",
    "\n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "    return datetime.strptime('190'+x, '%Y-%m')\n",
    "\n",
    "# convert time series into supervised learning problem\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return Series(diff)\n",
    "\n",
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series.values\n",
    "    # transform data to be stationary\n",
    "    diff_series = difference(raw_values, 1)\n",
    "    diff_values = diff_series.values\n",
    "    diff_values = diff_values.reshape(len(diff_values), 1)\n",
    "    # rescale values to -1, 1\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_values = scaler.fit_transform(diff_values)\n",
    "    scaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "    supervised_values = supervised.values\n",
    "    # split into train and test sets\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "    return scaler, train, test\n",
    "\n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons):\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    import pdb\n",
    "    pdb.set_trace()\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(y.shape[1]))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    # fit network\n",
    "    for i in range(nb_epoch):\n",
    "        model.fit(X, y, epochs=1, batch_size=n_batch, verbose=0, shuffle=False)\n",
    "        model.reset_states()\n",
    "    return model\n",
    "\n",
    "# make one forecast with an LSTM,\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    # make forecast\n",
    "    forecast = model.predict(X, batch_size=n_batch)\n",
    "    # convert to array\n",
    "    return [x for x in forecast[0, :]]\n",
    "\n",
    "# evaluate the persistence model\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n",
    "    forecasts = list()\n",
    "    for i in range(len(test)):\n",
    "        X, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_batch)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts\n",
    "\n",
    "# invert differenced forecast\n",
    "def inverse_difference(last_ob, forecast):\n",
    "    # invert first forecast\n",
    "    inverted = list()\n",
    "    inverted.append(forecast[0] + last_ob)\n",
    "    # propagate difference forecast using inverted first value\n",
    "    for i in range(1, len(forecast)):\n",
    "        inverted.append(forecast[i] + inverted[i-1])\n",
    "    return inverted\n",
    "\n",
    "# inverse data transform on forecasts\n",
    "def inverse_transform(series, forecasts, scaler, n_test):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "        # create array from forecast\n",
    "        forecast = array(forecasts[i])\n",
    "        forecast = forecast.reshape(1, len(forecast))\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        inv_scale = inv_scale[0, :]\n",
    "        # invert differencing\n",
    "        index = len(series) - n_test + i - 1\n",
    "        last_ob = series.values[index]\n",
    "        inv_diff = inverse_difference(last_ob, inv_scale)\n",
    "        # store\n",
    "        inverted.append(inv_diff)\n",
    "    return inverted\n",
    "\n",
    "# evaluate the RMSE for each forecast time step\n",
    "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n",
    "    for i in range(n_seq):\n",
    "        actual = [row[i] for row in test]\n",
    "        predicted = [forecast[i] for forecast in forecasts]\n",
    "        rmse = sqrt(mean_squared_error(actual, predicted))\n",
    "        print('t+%d RMSE: %f' % ((i+1), rmse))\n",
    "\n",
    "# plot the forecasts in the context of the original dataset\n",
    "def plot_forecasts(series, forecasts, n_test):\n",
    "    # plot the entire dataset in blue\n",
    "    pyplot.plot(series.values)\n",
    "    # plot the forecasts in red\n",
    "    for i in range(len(forecasts)):\n",
    "        off_s = len(series) - n_test + i - 1\n",
    "        off_e = off_s + len(forecasts[i]) + 1\n",
    "        xaxis = [x for x in range(off_s, off_e)]\n",
    "        yaxis = [series.values[off_s]] + forecasts[i]\n",
    "        pyplot.plot(xaxis, yaxis, color='red')\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "\n",
    "# load dataset\n",
    "series = read_csv('shampoo-sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "# configure\n",
    "n_lag = 1\n",
    "n_seq = 3\n",
    "n_test = 10\n",
    "n_epochs = 1500\n",
    "n_batch = 1\n",
    "n_neurons = 1\n",
    "# prepare data\n",
    "scaler, train, test = prepare_data(series, n_test, n_lag, n_seq)\n",
    "# fit model\n",
    "model = fit_lstm(train, n_lag, n_seq, n_batch, n_epochs, n_neurons)\n",
    "# make forecasts\n",
    "forecasts = make_forecasts(model, n_batch, train, test, n_lag, n_seq)\n",
    "# inverse transform forecasts and test\n",
    "forecasts = inverse_transform(series, forecasts, scaler, n_test+2)\n",
    "actual = [row[n_lag:] for row in test]\n",
    "actual = inverse_transform(series, actual, scaler, n_test+2)\n",
    "# evaluate forecasts\n",
    "evaluate_forecasts(actual, forecasts, n_lag, n_seq)\n",
    "# plot forecasts\n",
    "plot_forecasts(series, forecasts, n_test+2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
